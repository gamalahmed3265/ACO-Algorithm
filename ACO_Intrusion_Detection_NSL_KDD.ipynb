{"metadata":{"colab":{"collapsed_sections":[],"name":"Intrusion_Detection_NSL_KDD.ipynb","provenance":[]},"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.10.10","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"## Ant Colony Optimization algorithm  (ACOA) as an optimizer for training a NN","metadata":{}},{"cell_type":"code","source":"# importing required libraries\nimport numpy as np\nimport pandas as pd\nimport math\nimport pickle # saving and loading trained model\nfrom os import path\n\n# importing required libraries for normalizing data\nfrom sklearn import preprocessing\nfrom sklearn.preprocessing import StandardScaler\n\n# importing library for plotting\nimport matplotlib.pyplot as plt\n\n# importing library for support vector machine classifier\nfrom sklearn.svm import SVC\n# importing library for K-neares-neighbor classifier\nfrom sklearn.neighbors import KNeighborsClassifier\n# importing library for Linear Discriminant Analysis Model\nfrom sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n# importing library for Quadratic Discriminant Analysis Model\nfrom sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis\n\nfrom sklearn import metrics\nfrom sklearn.metrics import accuracy_score # for calculating accuracy of model\nfrom sklearn.model_selection import train_test_split # for splitting the dataset for training and testing\nfrom sklearn.metrics import classification_report # for generating a classification report of model\n\nfrom sklearn.metrics import precision_score\nfrom sklearn.metrics import recall_score\nfrom sklearn.metrics import f1_score\n\nfrom sklearn.metrics import roc_auc_score\nfrom sklearn.metrics import roc_curve, auc\n\nfrom keras.layers import Dense # importing dense layer\nfrom keras.models import Sequential #importing Sequential layer\nfrom keras.models import model_from_json # saving and loading trained model\n\nfrom keras.layers import LSTM\nfrom keras.layers import Input\nfrom keras.models import Model\n\n# representation of model layers\nfrom keras.utils.vis_utils import plot_model\nfrom numpy.random import choice as np_choice\nimport copy","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"path=\"datasets/bin_data.csv\"\nbin_data=pd.read_csv(path)\nbin_data","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X = bin_data.iloc[:,0:93].values # dataset excluding target attribute (encoded, one-hot-encoded,original)\nY = bin_data[['intrusion']].values # target attribute","metadata":{"id":"Imkkjtu_Fnzr","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# splitting the dataset 75% for training and 25% testing\nX_train, X_test, y_train, y_test = train_test_split(X,Y, test_size=0.25, random_state=42)","metadata":{"id":"gNx0tOV7FwfO","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def Fobj():\n    mlp = Sequential() # creating model\n    # adding input layer and first layer with 50 neurons\n    # X_train.shape is (94479, 93)\n    mlp.add(Dense(units=33, input_dim=X_train.shape[1], activation='relu'))#50x93=4650\n    # output layer with sigmoid activation\n    mlp.add(Dense(units=1,activation='sigmoid'))\n    return mlp\n# len(Fobj().get_weights()[1])\n# Fobj().get_weights()[0].flatten().shape[0]","metadata":{"id":"9-5ChLnLEVqn","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"mlp=Fobj()\n# defining loss function, optimizer, metrics and then compiling model\nmlp.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\ndim=0;\nfor i in range(len(mlp.get_weights())):# 4 for 4 array \n    dim=dim+mlp.get_weights()[i].flatten().shape[0]# 4650+50+50+1=4751\n# dim = 4751\n# print(math.sqrt(dim))\nN=20# Population size\nX=np.zeros((N,dim)) #(20, 4751)\n# print(X[:,:3])\nFit=np.ones(N)*float('inf') #(20, 1)\n# print(Fit.reshape(N,1))\nbF=float('inf')\nbPos=[];\n# print(len(mlp.get_weights()))#4\n# len(mlp.layers)#2","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# get wights and bias Y=XW+bias \ndef setWeights(X):\n    Len=0\n    # X is each row data in datasets X and len is 4751\n    # len(mlp.layers is 2 each contain 4 array\n    for i in range(0,len(mlp.layers)):\n#         get Wights first from layers\n        a=mlp.layers[i].get_weights()[0].shape\n        weights=X[Len:(Len+mlp.layers[i].get_weights()[0].flatten().shape[0])]\n        weights=np.reshape(weights,a) \n        Len=Len+mlp.layers[i].get_weights()[0].flatten().shape[0]\n#         get bias first from layers\n        a=mlp.layers[i].get_weights()[1].shape\n        bias=X[Len:(Len+mlp.layers[i].get_weights()[1].flatten().shape[0])]\n        bias=np.reshape(bias,a)\n        Len=Len+mlp.layers[i].get_weights()[1].flatten().shape[0]\n        mlp.layers[i].set_weights([weights,bias])\n#         print(len(mlp.get_weights()))#4\n#         len(mlp.layers)#4\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def check_boundaries(X):\n    for j in range(dim):\n        if X[j]>U_bounds:\n            X[j]=(L_bounds+np.random.random(1)*(U_bounds-L_bounds))\n        elif  X[j]<L_bounds:\n            X[j]=(L_bounds+np.random.random(1)*(U_bounds-L_bounds))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"L_bounds=0;\nU_bounds=1;\ndef initialization():\n    for i in range(N):\n        for j in range(dim):\n            X[i][j]=(L_bounds+np.random.rand(1)*(U_bounds-L_bounds))\n        setWeights(X[i,:])\n        print(\"------------Solution \",i,\"-------------\")\n        score=mlp.evaluate(X_train, y_train, verbose = 1) \n        Fit[i]= score[0]#loss\n        print(score[1])#accurcy\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#get numner node visted and probility\ndef pick_move(distances,pheromone, visited,alpha=1,beta=1):\n    all_inds = range(len(distances))\n    pheromone = np.copy(pheromone)\n    # print(pheromone)\n    pheromone[list(visited)] = 0\n    # print(pheromone[list(visited)])\n    row = pheromone ** alpha * (( 1.0 / distances) ** beta)\n    # h=f\"\"\"\n    # {pheromone} ** {alpha} * (( 1.0 / {dist}) ** {beta})\n    # \"\"\"\n    # print(h)\n    # print(row)\n    norm_row = row / row.sum()#probility\n    # print(norm_row)\n    move = np_choice(all_inds, 1, p=norm_row)[0]\n    return move #3","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#get path each node\ndef gen_path(distances,pheromone,start=0):\n    path = []\n    visited = set()\n    visited.add(start)\n    prev = start\n    for i in range(len(distances) - 1):\n        move = pick_move(distances[prev],pheromone[prev], visited)\n        path.append((prev, move))\n        prev = move\n        visited.add(move)\n        # print(f\"move: {move} prev:{prev} visited: {visited}\")\n        # print(f\"-- path {path}\")\n    path.append((prev, start)) # going back to where we started   \n    # print(f\"path {path}\")\n    return path","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#get distance each node\ndef gen_path_dist(distances,path):\n    total_dist = 0\n    # get values shortest path after\n    total_value=list()\n    for ele in path:\n          total_value.append(distances[ele])\n          total_dist +=distances[ele]\n    #     print(f\"{ele} : {distances[ele]}\")\n    # print(f\"total_dist: {total_dist}\")\n    return total_dist,total_value","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#get distance each node and path in all_path\ndef gen_all_paths(distances,pheromone,n_ants):\n    all_paths = []\n    for i in range(n_ants):\n        path = gen_path(distances,pheromone)\n        genPathDist,_=gen_path_dist(distances,path)\n        all_paths.append((path, genPathDist))\n    return all_paths","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def spread_pheronome(distances,pheromone ,all_paths, n_best, shortest_path):\n    # sorted the paths by distance\n    sorted_paths = sorted(all_paths, key=lambda x: x[1])\n    # print(f\"sorted_paths : {sorted_paths}\")\n    for path, dist in sorted_paths[:n_best]:\n        for move in path:\n            #update pheromone values \n            pheromone[move] += 1.0 / distances[move]\n    return pheromone","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def run(distances, n_ants, n_best, n_iterations, decay):\n    all_inds = range(len(distances))\n    pheromone = np.ones(distances.shape) / len(distances)\n    shortest_path = None\n    all_time_shortest_path = (\"placeholder\", np.inf)\n    for i in range(n_iterations):\n        #get distance each node and path in all_path\n        all_paths = gen_all_paths(distances,pheromone,n_ants)\n        # print(f\"all_paths : {all_paths}\")\n        pheromone=spread_pheronome(distances,pheromone ,all_paths, n_best, shortest_path=shortest_path)\n        shortest_path = min(all_paths, key=lambda x: x[1])\n#         print (shortest_path)\n        # swap the smallest value of shortest_path\n        if shortest_path[1] < all_time_shortest_path[1]:\n            all_time_shortest_path = shortest_path            \n        pheromone = pheromone * decay \n    return all_time_shortest_path","metadata":{"trusted":true},"execution_count":null,"outputs":[]}]}